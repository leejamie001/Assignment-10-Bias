{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b74d858",
   "metadata": {},
   "source": [
    "Hello! My hypothesis was that the Perspective would mark capitalized content, content that contained the negative context, and content that contained some type of threat/allusion to death as more toxic than other content that did not contain those things. To get more specific I would say that I thought that Perspective would mark content with allusion to harm as the most toxic.  I had three sets of sentences where I varied those factors. The first set was a more \"positive\" sentence that I then modified by adding either a curse word, capitalizing the sentence, adding a reference to harm, or a combination of such. The final sentence of each set was a combination of a curse word, reference to harm, and capitalization. I repeated this with a more \"negative\" base sentence as well as a sentence that was more vague. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7352a37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 36, 'score': {'value': 0.014953613, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.014953613, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: I loved your opinion in this article\n",
      "Toxicity score: 0.014953613\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 45, 'score': {'value': 0.45761138, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.45761138, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: I fucking loved your opinion in this article.\n",
      "Toxicity score: 0.45761138\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 44, 'score': {'value': 0.51980776, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.51980776, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: I FUCKING LOVED YOUR OPINION IN THIS ARTICLE\n",
      "Toxicity score: 0.51980776\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 64, 'score': {'value': 0.8540474, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.8540474, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: I loved your opinion in this article you should go kill yourself\n",
      "Toxicity score: 0.8540474\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 72, 'score': {'value': 0.93383175, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.93383175, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: I fucking loved your opinion in this article you should go kill yourself\n",
      "Toxicity score: 0.93383175\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 72, 'score': {'value': 0.944597, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.944597, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: I FUCKING LOVED YOUR OPINION IN THIS ARTICLE YOU SHOULD GO KILL YOURSELF\n",
      "Toxicity score: 0.944597\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 36, 'score': {'value': 0.50789946, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.50789946, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: I hated your opinion in this article\n",
      "Toxicity score: 0.50789946\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 45, 'score': {'value': 0.90451443, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.90451443, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: I fucking hated your opinion in this article.\n",
      "Toxicity score: 0.90451443\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 44, 'score': {'value': 0.944597, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.944597, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: I FUCKING HATED YOUR OPINION IN THIS ARTICLE\n",
      "Toxicity score: 0.944597\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 64, 'score': {'value': 0.91625386, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.91625386, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: I hated your opinion in this article you should go kill yourself\n",
      "Toxicity score: 0.91625386\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 72, 'score': {'value': 0.968226, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.968226, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: I fucking hated your opinion in this article you should go kill yourself\n",
      "Toxicity score: 0.968226\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 72, 'score': {'value': 0.9749944, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.9749944, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: I FUCKING HATED YOUR OPINION IN THIS ARTICLE YOU SHOULD GO KILL YOURSELF\n",
      "Toxicity score: 0.9749944\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 18, 'score': {'value': 0.2412222, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.2412222, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: Wow this was crazy\n",
      "Toxicity score: 0.2412222\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 18, 'score': {'value': 0.32020867, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.32020867, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: WOW THIS WAS CRAZY\n",
      "Toxicity score: 0.32020867\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 33, 'score': {'value': 0.85173553, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.85173553, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: Wow this was crazy you should die\n",
      "Toxicity score: 0.85173553\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 41, 'score': {'value': 0.96069145, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.96069145, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: Wow this was crazy you should fucking die\n",
      "Toxicity score: 0.96069145\n",
      "\n",
      "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 41, 'score': {'value': 0.968226, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.968226, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
      "Comment: WOW THIS WAS CRAZY YOU SHOULD FUCKING DIE\n",
      "Toxicity score: 0.968226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "API_KEY = \"my-api-key\"\n",
    "TOXICITY_API_URL = \"https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze\"\n",
    "\n",
    "comments = [\n",
    "    \"I loved your opinion in this article\",\n",
    "    \"I fucking loved your opinion in this article.\",\n",
    "    \"I FUCKING LOVED YOUR OPINION IN THIS ARTICLE\",\n",
    "    \"I loved your opinion in this article you should go kill yourself\",\n",
    "    \"I fucking loved your opinion in this article you should go kill yourself\",\n",
    "    \"I FUCKING LOVED YOUR OPINION IN THIS ARTICLE YOU SHOULD GO KILL YOURSELF\",\n",
    "    \n",
    "    \"I hated your opinion in this article\",\n",
    "    \"I fucking hated your opinion in this article.\",\n",
    "    \"I FUCKING HATED YOUR OPINION IN THIS ARTICLE\",\n",
    "    \"I hated your opinion in this article you should go kill yourself\",\n",
    "    \"I fucking hated your opinion in this article you should go kill yourself\",\n",
    "    \"I FUCKING HATED YOUR OPINION IN THIS ARTICLE YOU SHOULD GO KILL YOURSELF\",\n",
    "    \n",
    "    \"Wow this was crazy\",\n",
    "    \"WOW THIS WAS CRAZY\",\n",
    "    \"Wow this was crazy you should die\",\n",
    "    \"Wow this was crazy you should fucking die\",\n",
    "    \"WOW THIS WAS CRAZY YOU SHOULD FUCKING DIE\",\n",
    "\n",
    "]\n",
    "\n",
    "def get_toxicity_score(comment):\n",
    "    data = {\n",
    "        \"comment\": {\"text\": comment},\n",
    "        \"requestedAttributes\": {\"TOXICITY\": {}},\n",
    "        \"doNotStore\": True\n",
    " }\n",
    "    params = {\"key\":\"my-api-key\"}\n",
    "    response = requests.post(TOXICITY_API_URL, params=params, json=data)\n",
    "    response_dict = json.loads(response.content)\n",
    "    print(response_dict)  # Print response for debugging\n",
    "    return response_dict[\"attributeScores\"][\"TOXICITY\"][\"summaryScore\"][\"value\"]\n",
    "\n",
    "for comment in comments:\n",
    "    score = get_toxicity_score(comment)\n",
    "    print(f\"Comment: {comment}\\nToxicity score: {score}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f5623",
   "metadata": {},
   "source": [
    "From the tests, it seemed that Perspective would indeed increase the toxicity score when changes like capitalization, allusion to harm, negative context, and curse words were added. In the positive \"I loved your opinion in this article\" sentence example, the normal sentence had a toxicity score of 0.014953613 but spiked when I added a curse word (0.45761138) and then rose even more when I capitalized all of it (0.51980776). When I only added the reference to harm to the base sentence, it also rose a substansial amount (to 0.8540474), and only rose higher when I added a curse word (to 0.93383175) and then again when I capitalized the entire sentence (to 0.944597). \n",
    "\n",
    "This pattern of rising toxicity scores with these additions to the base sentence was also present when I started with the other base sentences. With the \"I hated your opinion in this article\" section, since it was a initial negative context, the toxicity score in this base sentence was higher as well (0.50789946 compared to the 0.014953613) and therefore the resulting additions had higer toxicity scores as well. \n",
    "\n",
    "The addition of a reference to harm caused the biggest jump in toxicity score (when only adding one change) in all 3 different base sentences, with the addition of a curse word (in this case \"fucking\") as the second biggest spike, and then capitalization as the next. \n",
    "\n",
    "I think that Perspectives performance in this test shows some bias in regard to deciding that capitalized sentences are more toxic than uncapitalized ones even though the content is the same. In some online communities, capitalization of whole sentences can be used in as a form of excitement/extra positiveness, but Perspective does not seem to think so. I think it may see excessive capitalization as aggressive and therefore assign a higher score to it. It also seemed to decide that the addition of a curse word even in a positive context like, \"I fucking loved your opinion in this article\", is more toxic which is perhaps not always the case. Curse words could be used in a non-toxic, excited way as well I guess. \n",
    "\n",
    "Overall, Perspective assigned toxicity scores in the way I thought they would, with the references to harm as the addition that would spike the toxicity score the most. It was interesting seeing how much \"toxicity value\" they assigned each new addition to the sentences though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89891fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
